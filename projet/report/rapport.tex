\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{appendix}
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{epstopdf}
\usepackage{stmaryrd}
\usepackage[]{algorithm2e}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\title{Project report for NPM3D : \\PCA based 3D point cloud classification}
\author{Leman FENG\\ Email: flm8620@gmail.com\\Website: lemanfeng.com}

\begin{document}
\maketitle
\section{Introduction}
After the mini-challenge of 3d point cloud classification in course, I decided to explore more on the subject of PCA based 3D points cloud classification. I read two articles \cite{weinmann2015} \cite{hackel2016fast} which present the state-of-art methods using PCA on neighborhoods. I will first summarize the two articles, then give some critical points of view, followed with a reproduction of method \cite{hackel2016fast} on a new dataset. At last I will give some new ideas on this subject together with test on a dataset.

\section{Article Summary}
The first article \cite{weinmann2015} provide a comprehensive comparison of different approaches in three aspects: neighborhood selection, feature design and classifier. And they proposed a better way of neighborhood selection. The second article \cite{hackel2016fast} focused on proposing a new pyramid grid sub-sampling and a new feature design. I will summarize the two article following the three aspects and their result on datasets.
\subsection{Neighborhood selection}
To tell the class of one point, the most natural way is to look at its neighborhoods. There are two ways of neighborhood selection : radius search and $k$ nearest neighbors. \cite{weinmann2015} and \cite{hackel2016fast} give different approaches. 

The first article \cite{weinmann2015} wants to find a optimal sizing of neighborhoods in the sense that the selection gives the most information on PCA features. They use $k$-NN from $k=10$ to $100$ and find the best $k$ maximizing the eigen-entropy:
\begin{equation}
E_\lambda = -\sum_{i=1}^{3} e_i \log e_i
\end{equation}
where $e_i = \lambda_i/\sum_{j=1}^{3} \lambda_j$.

And they use the best $k$ as the best and only scale to generate PCA features. They shown in tests that their eigen-entropy criterion gives better result in most of the time compared to different fixed $k$-NN and other optimal size criteria.

The second article \cite{hackel2016fast} use multi-scale neighborhood with $k$-NN. They firstly subsample the point cloud by voxel grids with grid sizing varying from 0.025m to 6.4m, each time multiplied by a factor of 2, totally 9 scales. They hope the sub-sampling can skip dense points for acquiring invariance to density. And this sub-sampling can help reduce the neighborhood querying time.

For each scale, they then fix $k=10$ for $k$-NN and do PCA on these 10 points.
\subsection{Feature design}
Most of features selected in two articles are based on eigenvalue and eigenvector of the PCA on neighborhoods. 
The second article \cite{hackel2016fast} uses 16 features. Shown in Table \ref{table:1}.
\begin{table}[h!]
	\centering
	\begin{tabular}{|c | c | c |} 
		\hline
		\multirow{9}{*}{Covariance}
		& Sum & $\sum_{i=1}^{3}\lambda_i$ \\ 
		& Omnivariance & $(\lambda_1 \lambda_2 \lambda_3)^{\frac{1}{3}}$ \\ 
		& Eigenentropy & $-\sum_{i=1}^{3}\lambda_i \log \lambda_i$ \\ 
		& Anisotropy & $(\lambda_1-\lambda_3)/\lambda_1$ \\ 
		& Planarity & $(\lambda_2-\lambda_3)/\lambda_1$ \\ 
		& Linearity & $(\lambda_1-\lambda_2)/\lambda_1$ \\ 
		& Surface Variation & $\lambda_3/(\lambda_1+\lambda_2+\lambda_3)$ \\ 
		& Sphericity & $\lambda_3/\lambda_1$ \\ 
		& Verticality & $1-|\langle [0,0,1], \mathbf{e}_1\rangle|$ \\ 
		\hline
		\multirow{4}{*}{Moment}
		& 1\textsuperscript{st} order, 1\textsuperscript{st} axis  & $\sum_{i\in \mathcal{N}}\langle \mathbf{p}_i-\mathbf{p}, \mathbf{e}_1\rangle$ \\ 
		& 1\textsuperscript{st} order, 2\textsuperscript{nd} axis & $\sum_{i\in \mathcal{N}}\langle \mathbf{p}_i-\mathbf{p}, \mathbf{e}_2\rangle$ \\ 
		& 2\textsuperscript{nd} order, 1\textsuperscript{st} axis & $\sum_{i\in \mathcal{N}}\langle \mathbf{p}_i-\mathbf{p}, \mathbf{e}_1\rangle^2$ \\ 
		& 2\textsuperscript{nd} order, 2\textsuperscript{nd} axis & $\sum_{i\in \mathcal{N}}\langle \mathbf{p}_i-\mathbf{p}, \mathbf{e}_2\rangle^2$ \\ 
		\hline
		\multirow{4}{*}{Height}
		& Vertical range & $z_{\text{max}}-z_{\text{min}}$ \\ 
		& Height below & $z-z_{\text{min}}$ \\ 
		& Height above & $z_{\text{max}}-z$ \\ 
		\hline
	\end{tabular}
	\caption{16 features used in \cite{hackel2016fast}}
	\label{table:1}
\end{table}
Most features of ``covariance'' in Table \ref{table:1} are also tested in \cite{weinmann2015}. The four ``Moment'' features take the neighborhood's offset into account, which can increase precision for boundary points, according to \cite{hackel2016fast}.

The ``Height'' features are measured in a cylindrical neighborhood. And there is a $5\%$ drop in precision if they are not used, according to \cite{hackel2016fast}.

In the first article \cite{weinmann2015}, they also tested other non-PCA feature such as 2D features. And from the test in \cite{weinmann2015}, we can conclude basically more features you use, better precision you get. 
\subsection{Classification}
\cite{weinmann2015} shown that Random Forest has good performance in most of time. And for this reason, \cite{hackel2016fast} only test on Random Forest.

\subsection{Dataset \& Result}
The first article \cite{weinmann2015} used the \textit{Oakland 3D Point Cloud Dataset} and \textit{Paris-rue-Madame database}. And the second article used \textit{Paris-rue-Madame database} and \textit{Paris-Rue-Cassette database}.

\textit{Oakland 3D Point Cloud Dataset} is already separated in training, validation and test sets. While \textit{Paris-rue-Madame database} is not separated. For \textit{Paris-rue-Madame database} and \textit{Paris-Rue-Cassette database}, both articles conduct a class re-balancing and randomly select 1000 points per class as training set, and rest points for test set.

\section{Criticism}
\subsection{Neighborhood selection}\label{sec:nb}
The selection of neighborhood is always the same story, either by querying points in a ball(or $k$-NN) with optimal radius(or $k$), or multiple balls with fixed sizes stacked together.

I think if the scale of dataset is known, such as 1 unit = 1 meter (and for most cases, it is. Because most data are laser scanned), then fixed radius querying makes sense because each type of object always gives the most useful information on a specific scale, for example, cars are always several meters long, pedestrians are always 1-2 meters tall.

But I don't think the multi-scaled concentric balls is the optimal solution. Most 3D features are related to PCA. The problem of PCA is that it basically represents the best ellipsoid that approximates your neighborhoods. If the ball is large, it covers more points, but an approximated ellipsoid is too abstract for a large coverage. And if the ball is small, PCA is a good local descriptor for shapes like line, plane, etc. But a small ball cover few points. There is always a dilemma of scale and detail.

\subsection{Feature design \& Classification}
\subsubsection{Unclear definition}
In both article, they all said three eigenvalues should be normalized such that the sum is one. But then they introduced the eigensum feature $\Sigma_\lambda = \sum_{i=1}^{3} \lambda_3$ AFTER this normalization. But they also use the eigenentropy term as a feature with the same symbol of eigenvalue as eigensum. For my reproduction, I will take eigensum as the sum of raw eigenvalues and use the normalized eigenvalues for all rest features.

\subsubsection{Hand craft features}\label{sec:features}
It seems that the key of point cloud classification is the design of your feature. More delicately crafted features you get, better the result is. I don't think this is the right way to go. PCA on neighborhood only provide 6 degrees of freedom, 3 for eigenvalues, 3 for a orthogonal basis of three eigenvectors. \cite{hackel2016fast} used 9 PCA features, and 8 of them only depend on eigenvalues. Which means you need to map your 3 eigenvalues into a higher dimension space that can make your random forest happy. 

I think the problem is, random forest is sensible to your feature design. The only thing a decision tree can do is to divide recursively your data by one feature value each time. So it's hard for a decision tree, given raw eigenvalues, to decide according to some function of them, for example, the linearity in function of eigenvalues: $L(\lambda_1,\lambda_2,\lambda_3) = (\lambda_1-\lambda_2)/\lambda_1$.

Why not let the machine to learn the best descriptors, instead of those hand craft features? Just like the Convolutional Neural Networks that can learn basic descriptors on images in the first several layers.

Besides, I don't think using the height information of points is a good idea. This features prevents translation invariance. Yes, most dataset has flat ground, and most pedestrians and cars are always on the ground. What about a car on a bridge going above a building ? The correct logic should bases on the space relationship. For example, a car-like object on a flat surface should be a car. A vertical cylinder under fuzzy points should be a tree, not a lamp-post. So for the following test, I deleted height features, which will decrease the precision by $5\%$ \cite{hackel2016fast}, according to their test.

\subsection{Dataset \& Result}
I don't agree with the way by which both article generate data from the \textit{Paris-rue-Madame database} and \textit{Paris-Rue-Cassette database}. They pick 1000 points per class randomly from point cloud, and the rest points become the test data. You cannot create training data and test data from the same point cloud. Simply because many testing points will have some neighbors belonging to 1000 points for training, and features for neighboring points are very similar. I'm afraid the $97\%$ overall precision in \cite{hackel2016fast} is just a over-fitting on training set.

\section{New ideas}
\subsection{Neighborhood selection}
\subsubsection{Voxel grid subsampling}
I will use the multi-scale voxel grid subsampling as \cite{hackel2016fast}, for the efficiency of querying and for a uniform density on point clouds. But I will use radius querying instead of $k$-NN because I think a fixed radius can well capture the geometry at specific scale. 

I adapt the size of voxel grid to the radius of query. In practice, I take voxel grid size as $\frac{1}{4}$ of the radius.

\subsubsection{Neighborhood's Neighborhood}
To solve the dilemma mentioned in Section \ref{sec:nb}, I propose a new method called \textit{Neighborhood's Neighborhood}. Instead of querying only by one ball centered at the point $\mathbf{p}$ to classify, I query 6 more balls surrounding the center ball : 

Let $\{\mathbf{e}_i\}_{i}$ be the three eigenvectors from result of PCA of the center ball, corresponding to three eigenvalues: $\{\lambda_i\}_{i}$ and $\lambda_1 \geq \lambda_2 \geq \lambda_3$. Let $R$ be the radius of the center ball. Then the 6 surrounding balls are centered at:
\begin{equation}
\mathbf{p}\pm \mu R \mathbf{e}_i,\quad i=1,2,3
\end{equation}
and they have the same radius as the center ball. In practice, I take $\mu=1.5$. A example in 2D is shown in Fig \ref{fig:nbnb}.

\begin{figure}[h]
	\centering
	\includegraphics[width=5cm]{nbnb.png}
	\caption{Neighborhood's neighborhood}
	\label{fig:nbnb}
\end{figure}

I use in total 4 scales, from 0.1m to 2.7m with a multiplication factor of 3 instead of 2 because with surrounding balls, each scale covers more points.

\subsection{Feature design}
I don't want to design any complicated features manually. I want to provide directly raw features from PCA, and then feed it into a convolutional neural network.

Let's define $\{\lambda_i\}_{i}$ and $\lambda_1 \geq \lambda_2 \geq \lambda_3$ as the raw eigenvalues from PCA. And $\Sigma_\lambda = \sum_{j=1}^{3}\lambda_j$ And $l_i = \lambda_i / \Sigma_\lambda$ as normalized eigenvalues.

Recall that $\Sigma_\lambda$ are actually the sum of squared distance to centroid of all points. So if we want to include $\Sigma_\lambda$ as a raw feature, it's better to normalize it as ${\Sigma_\lambda}/{R^2}$

For eigenvectors, the only feature I want to take is the verticality : 

$$\frac{2\arcsin(|\langle [0,0,1], \mathbf{e}_1\rangle|)}{\pi} \in [0, 1]$$

At last, for all balls at the same scale, I add the "density ratio to center ball" which is the ratio between number of neighbors $N$ in ball compared to which in center ball $N_c$. For consistency, this value is set to 1.0 for center ball.

For summary, the raw features I take are : 
\begin{table}[h!]
	\centering
	\begin{tabular}{| c | c |} 
		\hline
		Density ratio to center ball & $N / N_c$\\
		\hline
		Normalized eigensum & ${\Sigma_\lambda}/{R^2}$ \\ 
		\hline
		\multirow{3}{*}{Normalized Eigenvalues}
		& $\lambda_1/\Sigma_\lambda$ \\
		& $\lambda_2/\Sigma_\lambda$ \\
		& $\lambda_3/\Sigma_\lambda$ \\
		\hline
		Verticality & $2\arcsin(|\langle [0,0,1], \mathbf{e}_1\rangle|) / \pi$\\
		\hline
	\end{tabular}
	\caption{features}
	\label{table:2}
\end{table}

There are 4 scales, 7 balls for each scale and 6 features for each ball. In total 168 features. Compared to method in \cite{hackel2016fast}, there are $k$-NN in 9 scales from 0.025m to 6.25m, and 13 features for each scale, in total 117 features. 


\subsection{Classification}
I test three different classifiers, the first one is random forest. I choose 50 trees and 30 as depth, as recommended in \cite{hackel2016fast}.

The second one is Multi-Layer Percepton. I use 2 layers and the intermediate layer has 30 units. 

I propose a convolutional neural network as the third classifier:

\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{cnn.png}
	\caption{Convolutional Neural Network}
	\label{fig:cnn}
\end{figure}

Note ``n\_scale'', ``n\_ball'' and ``n\_feature'' as the number of scale, number of balls for each scale and number of features for each ball. 

The input is a matrix with size (n\_scale*n\_ball, n\_feature). Each row is a feature vector for one ball. 

Then I apply a small MLP on all feature vectors to map the feature space to another space with dimension ``n2'' (Layer ``L1''). The weight of this MLP is shared among all features, which is different from traditional MLP. This convolutional-like small MLP let the network to learn the best function of features.

Layer ``L1'' is reshaped to ``L2'', a 3D tensor of size (n\_scale, n\_ball, n3). Each floor of ``L2'' represent features in all balls at same scale.

A convolution is apply to ``L2'' with kernel size (1, n3, n\_ball), and in total m1 kernels. The idea of convolution is that a useful MLP for one scale are probably useful for other scales. For each scale, this MLP maps all features into a vector of length m1.

Finally, ``L3'' is full connected to the output.



\subsection{Dataset} \label{sec:dataset}
\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{Lille1_cut.png}
	\caption{Data separation for Lille1.ply}
	\label{fig:lill1}
\end{figure}

I choose the Paris-Lill1-3D dataset \cite{2017arXiv171200032R}. The dataset comes with 3 files: Lille1.ply, Lille2.ply and Paris.ply. I only use the first file.

To avoid data overlapping which causes false high precision in \cite{hackel2016fast} and \cite{weinmann2015}, I demand that training data and test data shall not come from the same part of the point cloud. To assure the data separation, I cut the point cloud manually into two parts A and B as shown in Fig \ref{fig:lill1}.


I didn't cut it directly into 2 parts, but pick specific regions. Because I need assure that we have similar structures in both part. For example, the left most street is cut in half through the road because the shape of boder tree there doesn't appear in other parts. So I separate the left/right hand sides of road into two part.

The dataset come with very detailed class labels on each point using the \textit{class tree} of iQmulus/Terramobilita benchmark. I did a remapping of class labels into 11 simplified classes, and ignore some classes with too few samples. The remapping is shown in Table \ref{table:class}.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|c|} 
		\hline
		My class & Description & Original class & Description\\
		\hline
		\multirow{3}{*}{0} & \multirow{3}{*}{Unknown} &
		(All other classes) & (All other classes)\\
		& & 000000000  & unclassified \\
		& & 100000000  & other \\
		\hline
		\multirow{4}{*}{1} & \multirow{4}{*}{Ground} &
		202020000 & road \\
		& & 202030000 & sidewalk \\
		& & 202050000 & island \\
		& & 202060000 & vegetal ground\\
		\hline
		2 & Building & 203000000 & Building \\
		\hline
		3 & Bollard & 302020300 & Bollard \\
		\hline
		4 & Floor lamp & 302020400 & Floor lamp \\
		\hline
		5 & Traffic light & 302020500 & Traffic light \\
		\hline
		\multirow{2}{*}{6} & \multirow{2}{*}{Sign} &
		302020600  & Traffic sign \\
		& & 302020700  & Signboard \\
		\hline
		7 & Roasting & 302030300 & Roasting \\
		\hline
		8 & Wire & 302030600 & Wire \\
		\hline
		9 & 4+ Wheels & 303040*** & 4+ Wheels \\
		\hline
		10 & Trash can & 303050500 & Trash can \\
		\hline
		\multirow{3}{*}{11} & \multirow{3}{*}{Natural} &
		    304000000  & Natural\\
		& & 304020000  & Tree \\
		& & 304040000  & Potted plant \\
		\hline
	\end{tabular}
	\caption{Remapping of class}
	\label{table:class}
\end{table}

\subsection{Result}
I use data part A for training and part B for testing. In part A, I pick as most 10000 samples per class for traning. And as most 20000 per class in part B for testing. Number of training and testing samples per class is shown in Table \ref{table:samples}.

\begin{table}[h!]
	\centering
	\makebox[\textwidth][c]{
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} 
		\hline
		& ground & building & bollard & floor lamp & traffic light & sign & roasting & wire & 4+ wheels & trash can & natural\\
		\hline
		Training & 20000 & 20000 & 17540 & 20000 & 17835 & 20000 & 20000 &  1973 & 20000 & 20000 & 20000 \\
		\hline
		Testing  & 20000 & 20000 & 17256 & 20000 &  7987 & 20000 & 20000 & 12211 & 20000 & 20000 & 20000 \\
		\hline
	\end{tabular}
	}
	\caption{Number of samples per class}
	\label{table:samples}
\end{table}

I tested both the method in \cite{hackel2016fast} and my \textit{Neighborhood's Neighborhood} method. But I removed all height features in \cite{hackel2016fast} for reason stated in Section \ref{sec:features}. 

Since the number of features are different in two methods, I tried many times and picked the best structure I can get for each method. It seems that the method in \cite{hackel2016fast} suffers easily from overfitting than my method. For this reason, I changed MLP to Single Layer Perceptron for method in \cite{hackel2016fast}, and I use only one convolutional layer for CNN. The structures of CNN for two methods are described by parameters and are shown in Table \ref{table:param}. 

The training curves for two Neural Networks and for the two methods are shown in Fig \ref{fig:timo_curve} and Fig \ref{fig:leman_curve}.


\begin{table}[h!]
	\centering
	\makebox[\textwidth][c]{
		\begin{tabular}{|c|c|c|c|c|c|c|} 
			\hline
			& \multicolumn{4}{|c|}{CNN} & \multicolumn{2}{|c|}{MLP}\\
			\hline 
			& n1 & n2 & m1 & parameters & layers & parameters \\
			\hline
			method in \cite{hackel2016fast} & 6 & (removed) & (removed) & 744 & Single layer & 1416 \\
			\hline
			My method & 30 & 10 & 6 & 1246 & two layers, width = 30 & 5442\\
			\hline
		\end{tabular}
	}
	\caption{Parameters of convolutional neural network structure}
	\label{table:param}
\end{table}

The code is available online at \href{https://github.com/flm8620/nuage_de_points/tree/master/projet}{my github}.

Feature generation is coded in C++ in the folder ``src''. Training and testing is coded in python in folder ``code''.

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{Figure_timo.png}
	\caption{Training curve of method in \cite{hackel2016fast}}
	\label{fig:timo_curve}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{Figure_leman.png}
	\caption{Training curve of my method \textit{Neighborhood's Neighborhood}}
	\label{fig:leman_curve}
\end{figure}

\begin{table}[h!]
	\centering
	\makebox[\textwidth][c]{
		\begin{tabular}{|c|c c c|c c c|c c c|} 
			\hline
			 & \multicolumn{3}{|c|}{Random forest} & \multicolumn{3}{|c|}{Single Layer Percepton} & \multicolumn{3}{|c|}{Convolutional}\\
			\hline
			 & precision & recall & f1-score & precision & recall & f1-score & precision & recall & f1-score\\
			\hline
ground			& 93.8 & 96.9 & 95.4 & 95.3 & 95.0 & 95.1 & 95.3 & 95.1 & 95.2 \\
building		& 48.1 & 91.2 & 62.9 & 74.2 & 81.1 & 77.5 & 75.2 & 80.4 & 77.7 \\
bollard			& 95.5 & 48.7 & 64.5 & 84.5 & 62.9 & 72.1 & 84.7 & 74.7 & 79.4 \\
floor lamp		& 47.1 & 74.4 & 57.7 & 56.3 & 71.4 & 62.9 & 54.4 & 73.7 & 62.6 \\
traffic light	& 70.4 & 02.7 & 05.2 & 72.4 & 59.0 & 65.0 & 72.6 & 55.4 & 62.8 \\
sign			& 26.7 & 10.4 & 14.9 & 32.7 & 27.3 & 29.8 & 34.1 & 29.7 & 31.8 \\
roasting		& 36.5 & 49.3 & 42.0 & 48.4 & 63.7 & 55.0 & 47.1 & 60.9 & 53.1 \\
wire			& 99.4 & 32.9 & 49.5 & 86.5 & 48.4 & 62.1 & 81.2 & 58.3 & 67.8 \\
4+ wheels		& 70.8 & 88.1 & 78.5 & 78.5 & 76.5 & 77.5 & 80.8 & 74.0 & 77.2 \\
trash can		& 49.1 & 23.1 & 31.4 & 48.3 & 45.7 & 47.0 & 53.3 & 44.3 & 48.4 \\
natural			& 38.2 & 82.4 & 52.2 & 52.0 & 75.3 & 61.5 & 54.4 & 70.9 & 61.6 \\
\hline                                                      
avg / total		& 61.4 & 54.6 & 50.4 & 66.3 & 64.2 & 64.1 & \textbf{66.6} & \textbf{65.2} & \textbf{65.2} \\
			\hline
			\end{tabular}
	}
	\caption{Test Result with method \cite{hackel2016fast}}
	\label{table:timo}
\end{table}
  
\begin{table}[h!]
	\centering
	\makebox[\textwidth][c]{
		\begin{tabular}{|c|c c c|c c c|c c c|} 
			\hline
			& \multicolumn{3}{|c|}{Random forest} & \multicolumn{3}{|c|}{MLP} & \multicolumn{3}{|c|}{Convolutional}\\
			\hline
			& precision & recall & f1-score & precision & recall & f1-score & precision & recall & f1-score\\
			\hline
			ground			& 98.8 & 99.0 & 98.9 & 98.1 & 98.8 & 98.4 & 98.0 & 97.7 & 97.8 \\
			building		& 60.8 & 90.8 & 72.8 & 70.6 & 88.2 & 78.4 & 62.2 & 85.8 & 72.1 \\
			bollard			& 94.4 & 94.1 & 94.2 & 94.7 & 87.7 & 91.1 & 89.9 & 95.7 & 92.7 \\
			floor lamp		& 60.4 & 90.6 & 72.5 & 50.7 & 83.7 & 63.2 & 65.4 & 76.7 & 70.6 \\
			traffic light	& 88.1 & 20.9 & 33.8 & 65.5 & 26.1 & 37.4 & 75.2 & 63.8 & 69.1 \\
			sign			& 53.8 & 52.7 & 53.3 & 70.6 & 54.1 & 61.3 & 64.3 & 52.6 & 57.9 \\
			roasting		& 75.5 & 59.9 & 66.8 & 70.6 & 74.6 & 72.6 & 64.9 & 64.4 & 64.6 \\
			wire			& 99.7 & 57.9 & 73.2 & 97.2 & 61.5 & 75.3 & 89.2 & 68.9 & 77.7 \\
			4+ wheels		& 89.5 & 95.0 & 92.2 & 91.3 & 91.6 & 91.4 & 88.8 & 89.6 & 89.2 \\
			trash can		& 85.6 & 73.4 & 79.0 & 76.8 & 73.9 & 75.3 & 82.8 & 58.4 & 68.5 \\
			natural			& 58.5 & 87.5 & 70.1 & 62.8 & 84.6 & 72.1 & 62.9 & 78.1 & 69.7 \\
			\hline
			                                                                        
			avg / total		& \textbf{78.6} & 74.7 & 73.3 & 77.2 & 75.0 & 74.2 & 76.7 & \textbf{75.6} & \textbf{75.4} \\
			\hline
		\end{tabular}
	}
	\caption{Test Result with my method}
	\label{table:leman}
\end{table}

\section{Conclusion}
The method in \cite{hackel2016fast} isn't as good as they claimed ``97\%'' overall precision. Although I removed ``height'' features which they said will lead to a drop of 5\%, the overall precision didn't reached ``92\%''. The best overall precision ``66.6\%'' in my test is not given by Random forest, where I used same parameter as indicated in their paper, it's given by my new CNN.

This shows that their way to generate training and testing data from the same point cloud do have problems. To confirm this, I tried again with 1000 points per class for training and 19000 points per class for testing, all points are selected randomly only from the part A of point cloud. Then I tested with Random Forest, and get the claimed ``92\%'' overall accuracy (Table \ref{table:mixtesting}). So they really overfitted their method.

\begin{table}[h!]
	\centering
	\makebox[\textwidth][c]{
		\begin{tabular}{|c|c c c|} 
			\hline
			& \multicolumn{3}{|c|}{Random forest}\\
			\hline
			& precision & recall & f1-score \\
			\hline
			ground			& 98.0 & 94.9 & 96.4 \\
			building		& 89.9 & 84.5 & 87.1 \\
			bollard			& 96.6 & 98.2 & 97.4 \\
			floor lamp		& 95.0 & 96.7 & 95.8 \\
			traffic light	& 99.4 & 98.8 & 99.1 \\
			sign			& 95.5 & 95.5 & 95.5 \\
			roasting		& 86.5 & 85.0 & 85.8 \\
			wire			& 99.7 & 83.5 & 90.9 \\
			4+ wheels		& 87.9 & 94.3 & 91.0 \\
			trash can		& 92.2 & 98.1 & 95.1 \\
			natural			& 74.5 & 82.5 & 78.3 \\
			\hline                        
			avg / total		& 92.3 & 92.0 & 92.0 \\
			\hline
		\end{tabular}
	}
	\caption{Mixing training and testing data gives the claimed results in \cite{hackel2016fast}, but actually an overfitting}
	\label{table:mixtesting}
\end{table}

My \textit{Neighborhood's Neighborhood} method get a ``76\%-78\%'' overall precision/recall/f1-score. And from the results of two neural networks, my new CNN can achieve the same performance with fewer parameters, and no overfitting during the training, as shown in Fig \ref{fig:leman_curve}.

I think non-hand-crafted, raw features combined with CNN should be the good direction to go for future researches.

\bibliographystyle{unsrt}
\bibliography{sample}
\end{document}



































